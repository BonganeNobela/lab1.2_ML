{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395bb894-82f4-421c-b6f5-2b341f133daf",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 1.2 - Naive Bayes Pt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc237d7d-d710-4b56-ba46-6320c732e2b3",
   "metadata": {},
   "source": [
    "Edit the following to include the names of everyone in your group including you, or if you are not in a group \n",
    "- **Student Names:** name 1, name 2, ...\n",
    "- **Student Numbers:** student no 1, student no 2, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bc53ed-f58c-416b-8335-cfce12eb4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91c0aa-2f23-4b47-92df-a49e47efd505",
   "metadata": {},
   "source": [
    "# Section 1 - Digit Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76f38f-410f-4de2-8e3b-27acd2c3de81",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "**Do not change the below cells, but you must run them**\n",
    "\n",
    "\n",
    "Load data into pandas dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7adb2b-e831-476e-a1c4-327dcc5a09d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   ...  55  56  57  58  59  60  61  \\\n",
       "1245   0   0   0   1   1   0   0   0   0   0  ...   0   0   0   0   1   1   1   \n",
       "220    0   0   1   1   1   0   0   0   0   1  ...   0   0   0   1   1   1   0   \n",
       "1518   0   0   1   1   1   0   0   0   0   1  ...   0   0   0   1   1   1   1   \n",
       "438    0   0   0   1   1   1   1   0   0   0  ...   0   0   0   0   1   0   0   \n",
       "1270   0   0   1   1   1   0   0   0   0   1  ...   0   0   0   1   1   1   1   \n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "1130   0   0   1   1   1   1   0   0   0   1  ...   0   0   0   1   1   1   1   \n",
       "1294   0   0   1   1   1   0   0   0   0   0  ...   0   0   0   1   1   0   0   \n",
       "860    0   0   1   1   1   1   0   0   0   0  ...   0   0   0   1   1   1   1   \n",
       "1459   0   0   0   1   1   1   0   0   0   0  ...   0   0   0   0   1   1   0   \n",
       "1126   0   0   0   1   1   0   0   0   0   0  ...   0   0   0   0   1   1   0   \n",
       "\n",
       "      62  63  64  \n",
       "1245   1   0   6  \n",
       "220    0   0   9  \n",
       "1518   0   0   3  \n",
       "438    0   0   7  \n",
       "1270   0   0   2  \n",
       "...   ..  ..  ..  \n",
       "1130   0   0   3  \n",
       "1294   0   0   7  \n",
       "860    0   0   2  \n",
       "1459   0   0   7  \n",
       "1126   0   0   1  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data (digits) & labels as pandas dataframe object\n",
    "df = pd.read_csv(\"smalldigits.csv\", header=None)\n",
    "df = df.sample(frac=1, random_state=42, axis=0)  # Randomise dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd06fa7-cc5f-4b0b-bfee-802dfb8f9dd5",
   "metadata": {},
   "source": [
    "Create 90/10 train test split and convert to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f14e3f-7929-403f-9140-9be38b27a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_digits: \n",
      " [[0 0 0 ... 1 1 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "train_labels: \n",
      " [6 9 3 ... 8 0 4]\n"
     ]
    }
   ],
   "source": [
    "n_rows = df.shape[0]\n",
    "# Roughly 90/10 train-test split\n",
    "train_digits = df.iloc[:int(n_rows * 0.9), :-1].to_numpy()\n",
    "train_labels = df.iloc[:int(n_rows * 0.9), -1].to_numpy()\n",
    "\n",
    "print(\"train_digits: \\n\", train_digits)\n",
    "print(\"\\ntrain_labels: \\n\", train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70371746-901b-460d-833f-4becfc18ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_digits: \n",
      " [[0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "test_labels: \n",
      " [9 5 0 5 3 6 9 1 1 8 4 0 9 2 6 9 8 6 8 7 1 5 7 2 8 0 8 8 6 7 0 9 3 8 2 0 2\n",
      " 1 7 4 3 1 4 2 8 2 5 3 8 5 5 5 8 1 5 3 1 1 9 1 4 4 4 3 5 6 8 2 5 7 5 1 5 7\n",
      " 9 9 2 7 1 9 0 9 3 7 5 9 0 3 5 1 1 5 0 0 5 0 4 1 2 2 6 8 8 0 3 4 3 3 8 4 0\n",
      " 6 0 2 3 3 2 1 0 0 4 0 0 1 8 2 0 4 1 5 6 8 0 3 9 8 2 8 4 0 6 4 1 0 0 1 2 9\n",
      " 7 0 8 6 3 9 2 4 4 8 3 8 0 0 6 8 5 4 6 5 7 1 4 4 5 2 4 3 7 2 7 1]\n"
     ]
    }
   ],
   "source": [
    "test_digits = df.iloc[int(n_rows * 0.9):, :-1].to_numpy()\n",
    "test_labels = df.iloc[int(n_rows * 0.9):, -1].to_numpy()\n",
    "\n",
    "print(\"test_digits: \\n\", test_digits)\n",
    "print(\"\\ntest_labels: \\n\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00b79f-67fd-47fb-aa7f-bd586f2e62d7",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- `train_digits` - train features\n",
    "- `train_labels` - train labels\n",
    "- `test_digits` - test features\n",
    "- `test_labels` - test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f91d7-fa97-4d46-89bb-56d0c78026ae",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4baff3-bb42-4b27-895e-54141cce18c5",
   "metadata": {},
   "source": [
    "`vis_digit` can be used to visualise a given digit. This may be useful for debugging and/or your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b416cfec-ffa4-4c5e-a69e-4062b4d082e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vis_digit(digit):\n",
    "    plt.imshow(digit.reshape(8, 8), cmap=\"viridis\")\n",
    "\n",
    "print(f\"Label = {train_labels[0]}\")\n",
    "vis_digit(train_digits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25653dc-2098-44d6-beaa-151cea7f03a2",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9103ca-25f6-4e19-9bfb-1f3d4d7a93c2",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Compute the **prior** probabilities for each class. These values should be stored in the numpy array `priors`, with the prior for label 0 being at index 0 of the `priors`, label 1 being at index 1 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa2f440-38b6-4216-9e70-9b403d50a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09461967, 0.0995671 , 0.09833024, 0.10265925, 0.10018553,\n",
       "       0.10018553, 0.10451453, 0.10327767, 0.09400124, 0.10265925])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors = np.zeros(10)\n",
    "# TODO\n",
    "\n",
    "tot = len(train_labels)\n",
    "for i in range(10):\n",
    "    priors[i] = np.sum(train_labels == i) / tot\n",
    "\n",
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c469d2-9320-4a15-bf55-fc8de02fc870",
   "metadata": {},
   "source": [
    "#### Question 2 \n",
    "Compute the class conditionals with Laplacian smoothing and and assign their values to the numpy array `class_conditionals`. Set `k = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b41add-487d-493e-8c8b-752f0615ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_conditionals = np.zeros((10, 64))  # 10 classes, 64 features\n",
    "k = 1\n",
    "\n",
    "# TODO\n",
    "\n",
    "class_conditionals[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4935d65-724f-4180-97f5-1aaaa2a7c36e",
   "metadata": {},
   "source": [
    "##### Visualise class conditionals \n",
    "Below, for each class we are plotting the associated probabilities of each pixel (i.e. features). If your computation of the class conditionals is correct then the plots below should vaguelly look like the associated label.\n",
    "\n",
    "Think about why visualising the class conditional model in this way shows the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20f702-882c-40c7-9da5-80459d01c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 7))  # 1 row, 2 columns\n",
    "for i in range(5):\n",
    "    axes[0][i].imshow(class_conditionals[i].reshape(8, 8), cmap=\"viridis\")\n",
    "    axes[0][i].set_title(f\"label = {i}\")\n",
    "\n",
    "for i in range(5):\n",
    "    axes[1][i].imshow(class_conditionals[i+5].reshape(8, 8), cmap=\"viridis\")\n",
    "    axes[1][i].set_title(f\"label = {i+5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bb5ed-cd89-4ad9-afbc-1972ff3aa8d6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540533d-0c35-4f7e-8472-9bdc85cfc039",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Finish the function `calc_posterior` that computes $P(C|X)$, where $C$=`label` and $X$=`features`.\n",
    "- `features`: $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5601993-c76c-4771-a5c4-be5166c9f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_posterior(features):\n",
    "    # Calc P(X|C) for each C    \n",
    "    feat_class_conds = np.ones(10)  # feat_class_conds[0] corresponds to P(X|C=0)\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return p_c_x\n",
    "\n",
    "\n",
    "print(f\"Posterior probs for digit = {calc_posterior(test_digits[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b15e3-bccb-4623-ba5e-af590c1cc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_digits[0]:\")\n",
    "vis_digit(test_digits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f249d1b-0add-46b9-b8ba-5c080ce26a45",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896c985-610f-4d3b-9670-2ceef2455253",
   "metadata": {},
   "source": [
    "Finish the function `infer_class` that infers/predicts the most probable class for the given data `digit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8e272-4752-4b26-90bd-66f36cf37471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_class(digit):\n",
    "    # TODO\n",
    "    pred_label = ...\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "infer_ind = 0\n",
    "print(f\"Predicted label = {infer_class(test_digits[infer_ind])}; True label = {test_labels[infer_ind]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b552b-b59e-4033-9b83-c5d006f57f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_digits[{infer_ind}]:\")\n",
    "vis_digit(test_digits[infer_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f8148-eb53-4efe-ae53-7d7b1e253b55",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Create a confusion matrix using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca5ea0-672a-4452-a40a-c02543cfc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Don't modify\n",
    "print(confusion_matrix)\n",
    "plt.imshow(confusion_matrix)  # Plot heatmap of confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024ec06-a488-490f-82d7-e5dd6b0843eb",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a1a2a-8367-4426-a35a-ddfaf449575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = ...\n",
    "\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f41e7a-5eab-4b83-9ad3-de99dc9a5de7",
   "metadata": {},
   "source": [
    "# Section 2 - Naive Bayes with Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6aa2b-1212-4a23-a2fc-f21d53798c50",
   "metadata": {},
   "source": [
    "The file `banknote_authentication.csv` contains 100 examples of genuine (class=1) and forged (class=0) banknotes. These images were analysed with a wavelet transform tool that generated four continuous features: variance, skewness, curtosis and entropy (of each image). For each feature in both classes, you must fit a Gaussian distribution to that feature and use this to make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722db9b5-7779-424a-b303-540deb99dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv(\"banknote_authentication.csv\", sep=\";\")\n",
    "bank_df = bank_df.sample(frac=1, random_state=42)  # Randomise\n",
    "bank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee995b7-8fc6-4424-8e34-b0b0985535f4",
   "metadata": {},
   "source": [
    "## Data Analysis & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b19f9-c021-420c-ac8e-d3332f22938c",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**a)**\n",
    "Plot 8 seperate histograms: one for each variable for each class. These plots must be rendered in the provided matlotlib axes (`axs`). The top row should correspond to `class=0` and the bottom row should correspond to `class=1`. For each plot you must set the title of the axis to have the format `class={class}-{feature_name}`. Incorrect formating will lead to zero marks for this question.\n",
    "\n",
    "**Tip:** I'd recommend using the plotting library [Seaborn](https://seaborn.pydata.org/index.html) for this as it will make things easier, but you are also welcome to just use matplotlib directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3225dd-3e8e-4f83-99ab-379c0ae93987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using seaborn uncomment the below and run\n",
    "# !pip install seaborn\n",
    "# import seaborn as sns\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7ce6f-4246-49d5-bef4-ad4f3905c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(14, 7)) # Don't remove\n",
    "\n",
    "# TODO\n",
    "\n",
    "fig.tight_layout()  # Don't remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d1f56-2e2b-436d-aa9d-15b6759c79e4",
   "metadata": {},
   "source": [
    "**b)** Do these distributions look like Gaussian distributions? How well do you expect this to work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759e073-d099-4c04-a078-0f426616c5c5",
   "metadata": {},
   "source": [
    "*This is a Markdown Cell. Double click this text to edit.*\n",
    "\n",
    "Put your answer below:\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369253b2-19ef-42f6-af28-e3ca57bcfad3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e55e8-a97b-4704-87bd-ff05bedcecff",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b570c-e33c-4d11-afd0-0725bae8e343",
   "metadata": {},
   "source": [
    "First create an 80-20 train-test split. Note that we first randomised the dataframe so the data is already randomised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54480798-af51-4d95-8c50-801707fba331",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(bank_df.shape[0] * 0.8)\n",
    "\n",
    "print(\"#########\")\n",
    "print(\"# TRAIN #\")\n",
    "print(\"#########\")\n",
    "s2_train_features = bank_df.iloc[:split_index, :-1].to_numpy()\n",
    "s2_train_labels = bank_df.iloc[:split_index, -1].to_numpy()\n",
    "\n",
    "print(f\"first ten rows of s2_train_features = \\n {s2_train_features[:10]}\")\n",
    "print(f\"\\nfirst ten elements of s2_train_labels = \\n {s2_train_labels[:10]}\")\n",
    "\n",
    "print(\"\\n########\")\n",
    "print(\"# TEST #\")\n",
    "print(\"########\")\n",
    "s2_test_features = bank_df.iloc[split_index:, :-1].to_numpy()\n",
    "s2_test_labels = bank_df.iloc[split_index:, -1].to_numpy()\n",
    "\n",
    "print(f\"first ten rows of s2_test_features = \\n {s2_test_features[:10]}\")\n",
    "print(f\"\\nfirst ten elements of s2_test_labels = \\n {s2_test_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db49c8b-b541-4321-9b8f-52a1ab146781",
   "metadata": {},
   "source": [
    "**Note: If you do not train only on the training data you will lose significant marks or get zero for the preceeding questions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b5541-c99a-4549-a1bd-ea12285b2215",
   "metadata": {},
   "source": [
    "### Question 2 - Priors\n",
    "Calculate the class priors and set them to the numpy array `s2_priors`. Element 0 of the array should correspond to class=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4803a-a06f-4de5-925c-5eebb0ce287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_priors = np.zeros(2)\n",
    "\n",
    "# TODO\n",
    "\n",
    "s2_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad6b4c-e5e7-45a1-9d99-efb3c622710c",
   "metadata": {},
   "source": [
    "### Question 3 - Class Conditionals\n",
    "For each feature $x_i$ and class $c$ fit a gaussian distribution to the associated data and implement the function `s2_class_conditional_fn`. Note that you **must** implement the relevant equations yourself - do not just use in built methods for computing the mean, variance and what not.\n",
    "\n",
    "**TIP:** Use the relevant equations found in the lecture notes \"Lec 1.2 - More on Naive Bayes\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9d058-a72b-4fba-8c26-f02ff1a70ef9",
   "metadata": {},
   "source": [
    "**a)** Fit gaussian distributions to each feature and class $(x_i, c)$. I.e., compute the mean ($\\mu_{x_i, c}$) and variance ($\\sigma^2_{x_i, c}$) for each $(x_i, c)$. Store these values in the numpy arrays `s2_cc_mean` for the means, and `s2_cc_var` for the variance. The rows of these arrays must correspond to features, and the columns must correspond to classes. Note: $x_0$=\"variance\", $x_1$=\"skewness\", $x_2$=\"curtosis\" and $x_3$=\"entropy\". $c_0$=0 and $c_1$=1.\n",
    "\n",
    "The format of `s2_cc_mean` is as follows:\n",
    "`s2_cc_mean` = </br>\n",
    "\\[ </br>\n",
    "&emsp; \\[$\\mu_{x_0, c_0}$, $\\mu_{x_1, c_0}$, $\\mu_{x_2, c_0}$, $\\mu_{x_3, c_0}$],</br>\n",
    "&emsp; \\[$\\mu_{x_0, c_1}$, $\\mu_{x_1, c_1}$, $\\mu_{x_2, c_1}$, $\\mu_{x_3, c_1}$] </br>\n",
    "]\n",
    "\n",
    "The format of `s2_cc_var` follows similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908f846-0bab-4da8-9363-b3d776939c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_cc_mean = np.zeros((2, 4))\n",
    "\n",
    "# TODO\n",
    "\n",
    "s2_cc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7a4db-f8de-45f3-b02f-b89fe86eb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_cc_var = np.zeros((2,4))\n",
    "\n",
    "# TODO\n",
    "\n",
    "s2_cc_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3212c86-7d45-48f0-bdda-ae2ec46f74a9",
   "metadata": {},
   "source": [
    "**b)** Implement the function `s2_class_conditional_fn` which will compute $P(x_i | c)$. This function takes in the feature, class (class_label), mean and variance (var).\n",
    "- `feature`: $x_i$\n",
    "- `class_label`: $c$\n",
    "- `mean`: mean ($\\mu_{x_i, c}$) of associated gaussian distribution for $(x_i, c)$\n",
    "- `var`: variance ($\\sigma^2_{x_i, c}$) of associated gaussian distribution for $(x_i, c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26622bf3-7676-4805-99cb-5fbf179857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_class_conditional_fn(feature, class_label, mean, var):\n",
    "    cond_prob = ...   # i.e. P(x_i | c)\n",
    "    # TODO\n",
    "\n",
    "    return cond_prob\n",
    "\n",
    "tmp_feature = s2_train_features[0, 0]\n",
    "# tmp_class = 0\n",
    "print(f\"P(x_0={tmp_feature}|c={0}) = {s2_class_conditional_fn(tmp_feature, 0, s2_cc_mean[0, 0], s2_cc_var[0, 0])}\")\n",
    "print(f\"P(x_0={tmp_feature}|c={1}) = {s2_class_conditional_fn(tmp_feature, 1, s2_cc_mean[0, 1], s2_cc_var[0, 1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdd9fd-3ffb-44b5-8ff6-c73773d8cd4b",
   "metadata": {},
   "source": [
    "### Question 4 - Posterior Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec370fea-6b01-4fe9-b3ce-0542f1e213f5",
   "metadata": {},
   "source": [
    "Implement the function `s2_calc_posterior` that calculates the posterior probability of a given class based off given data. I.e. it should compute $P(c|x)$.\n",
    "- `feature`: $x$\n",
    "- `class_label`: $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10378391-a3b4-44b4-bee1-858d7ba73ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_calc_posterior(class_label, feature):\n",
    "    post_prob = ...\n",
    "    # TODO\n",
    "\n",
    "    return post_prob\n",
    "\n",
    "# Don't change\n",
    "print(f\"P(c=0 | x={s2_test_features[0]}) = {s2_calc_posterior(0, s2_test_features[0])}\")\n",
    "print(f\"P(c=1 | x={s2_test_features[0]}) = {s2_calc_posterior(1, s2_test_features[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d835f9e-94dc-4703-83e5-a8b99900bdf1",
   "metadata": {},
   "source": [
    "## Question 5 - Predict Class\n",
    "Implement the function `s2_infer_class`. Which should return the most probable class for the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019baeea-bbba-4414-8dd5-5fdf8f6d98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_infer_class(feature):\n",
    "    c = ...\n",
    "    # TODO\n",
    "    \n",
    "    return c\n",
    "\n",
    "print(f\"Inferred class for x={s2_test_features[0]} = {s2_infer_class(s2_test_features[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36bdcbe-1d51-41a4-a9e9-429dd6a2cd53",
   "metadata": {},
   "source": [
    "## Question 6 - Confusion Matrix & Accuracy\n",
    "**a)** Compute the confusion matrix using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243322f9-595a-4820-babb-bd7c2d38a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_confusion_matrix = np.zeros((2, 2))\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Don't modify\n",
    "print(s2_confusion_matrix)\n",
    "plt.imshow(s2_confusion_matrix)  # Plot heatmap of confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a369939-1695-40ea-b85f-4102965a6418",
   "metadata": {},
   "source": [
    "**b)** Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa9274-0659-448e-9a03-450a86337473",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_acc = ...\n",
    "\n",
    "# TODO\n",
    "\n",
    "s2_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdf695-04d9-41e0-85c6-6d1fed4dd8c5",
   "metadata": {},
   "source": [
    "**Does this accuracy align with what you expected based off how well (or not well) the data fits normal distributions?** (Note: Don't write the answer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490a135-b2dd-4090-b2d4-8efa1f635648",
   "metadata": {},
   "source": [
    "# \\[Optional for Bonus Marks] Section 3 - Harry Potter Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e05e3b-f013-4f34-aac4-df59d0ccc5db",
   "metadata": {},
   "source": [
    "We will now look at a more challenging text-based classification problem, namely to classify a page from a Harry Potter book into which of the seven books the page was taken from. The books can be found in the zip file hp books.zip and are text files where each page of a given book is a line in the text file. Note, all punctuation and capital letters have been removed from the file, so that only the words of the page remain to be used by our model.\n",
    "\n",
    "**Note:** Add and use mutliple code cells for each question to improve readibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6db3f3-3eda-474e-bb85-e409a28b6421",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Train an NB model using 80% of the data to train and the remaining 20% as test data. Use Laplace smoothing for your model. Report a confusion matrix of your results. Laplace smoothing is a simple way of avoiding 0 values in the class-conditional models (table of likelihoods). However, it may cause problems when many unique, infrequent words are added to the table (when multiplied together low likelihoods may still become 0 but too large a smoothing value will bias the model). In such a case even removing stop words* may not be enough. Thus, we will now smooth the table of likelihoods by adding a set value to each element of the table. The smoothing value used will now become a hyper-parameter for our algorithm, and so we will need to use a validation data set to find the correct value for the hyper-parameter.\n",
    "\n",
    "*stop words are 'unhelpful' frequent words such as 'and', 'the', 'at' and so on that are often removed from the data to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e7602-bd35-4892-bbdf-7555b73561d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fa196c3-63ed-49f5-9c84-fa5ade5e023b",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559f385-5ce8-4795-9e37-77a4413665bd",
   "metadata": {},
   "source": [
    "**a)** Adapt your code to use 80% of the data to train, 10% of the data as validation data and the remaining 10% as test data. Train separate NB classifiers using the values {$1 \\times 10^{-1}$, $1 \\times 10^{-2}$, $1 \\times 10^{-3}$, $1 \\times 10^{-4}$, $1 \\times 10^{-5}$, $1 \\times 10^{-6}$} to smooth the table of likelihoods. Train each model using the training data, and track its performance on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda250f3-28cd-42b9-8fac-030e5cb0ea41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722b3345-50aa-4704-8eaa-74d8803cda3f",
   "metadata": {},
   "source": [
    "**b)** Which model gave the best accuracy on validation data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b1b7d-10cf-424d-8c8a-2d2ae8729318",
   "metadata": {},
   "source": [
    "TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97d178-7b40-4815-8004-ad02631f1f0f",
   "metadata": {},
   "source": [
    "**c)** Does the choice of smoothing value have a big impact on the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b67f1-b1fc-458e-a763-231a241d9810",
   "metadata": {},
   "source": [
    "TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc391f8-f2ce-4e62-be31-655e281c8d93",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbea3b-bc02-4708-8c81-cd47fa72d0d3",
   "metadata": {},
   "source": [
    "Use the model which achieved the best validation accuracy and test it using the test data set. Report a confusion matrix of the results, as well as the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565c32e-8635-49c6-95b0-ac789cf66f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd1e3f0d-d8f4-4d18-aeab-fc393ee29430",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1516c-f1b0-4c21-8eb5-876495a38dbd",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix, which books would you say are most similar to each other (hint: look at which books are often confused with each other)? Do you think JK Rowling's writing style changed over time? Why else do you think certain books are more easily confused with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7cd071-9f6e-40cc-bf65-7d4437adf3d0",
   "metadata": {},
   "source": [
    "TODO: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d97cc6-e24f-46a2-85ad-e6db3ac03546",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
